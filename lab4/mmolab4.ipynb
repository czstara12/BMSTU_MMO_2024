{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стратегия:\n",
      "array([[0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25]])\n",
      "Алгоритм выполнился за 1000 шагов.\n",
      "Стратегия:\n",
      "array([[1.  , 0.  , 0.  , 0.  ],\n",
      "       [0.  , 0.  , 0.  , 1.  ],\n",
      "       [0.  , 0.  , 0.  , 1.  ],\n",
      "       [0.  , 0.  , 0.  , 1.  ],\n",
      "       [1.  , 0.  , 0.  , 0.  ],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.5 , 0.  , 0.5 , 0.  ],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.  , 0.  , 0.  , 1.  ],\n",
      "       [0.  , 1.  , 0.  , 0.  ],\n",
      "       [1.  , 0.  , 0.  , 0.  ],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.25, 0.25, 0.25, 0.25],\n",
      "       [0.  , 0.  , 1.  , 0.  ],\n",
      "       [0.  , 1.  , 0.  , 0.  ],\n",
      "       [0.25, 0.25, 0.25, 0.25]])\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "pygame is not installed, run `pip install gym[toy_text]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:283\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 283\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 128\u001b[0m\n\u001b[0;32m    124\u001b[0m     play_agent(agent)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 124\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m agent\u001b[38;5;241m.\u001b[39mprint_policy()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Проигрывание сцены для обученного агента\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43mplay_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 99\u001b[0m, in \u001b[0;36mplay_agent\u001b[1;34m(agent)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_agent\u001b[39m(agent):\n\u001b[0;32m     98\u001b[0m     env2 \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    100\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\wrappers\\time_limit.py:68\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:42\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:45\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_reset_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:192\u001b[0m, in \u001b[0;36menv_reset_passive_checker\u001b[1;34m(env, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFuture gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Checks the result of env.reset with kwargs\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    195\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:266\u001b[0m, in \u001b[0;36mFrozenLakeEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\czsta\\Desktop\\BMSTU_MMO_2024\\venv\\Lib\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:285\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpygame is not installed, run `pip install gym[toy_text]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     pygame\u001b[38;5;241m.\u001b[39minit()\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: pygame is not installed, run `pip install gym[toy_text]`"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "class PolicyIterationAgent:\n",
    "    '''\n",
    "    Класс, эмулирующий работу агента\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # Пространство состояний\n",
    "        self.observation_dim = 16\n",
    "        # Массив действий в соответствии с документацией\n",
    "        # https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
    "        self.actions_variants = np.array([0,1,2,3])\n",
    "        # Задание стратегии (политики)\n",
    "        # Карта 4х4 и 4 возможных действия \n",
    "        self.policy_probs = np.full((self.observation_dim, len(self.actions_variants)), 0.25)\n",
    "        # Начальные значения для v(s)\n",
    "        self.state_values = np.zeros(shape=(self.observation_dim))\n",
    "        # Начальные значения параметров\n",
    "        self.maxNumberOfIterations = 1000\n",
    "        self.theta=1e-6\n",
    "        self.gamma=0.99\n",
    "\n",
    "\n",
    "    def print_policy(self):\n",
    "        '''\n",
    "        Вывод матриц стратегии\n",
    "        '''\n",
    "        print('Стратегия:')\n",
    "        pprint(self.policy_probs)\n",
    "\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        '''\n",
    "        Оценивание стратегии \n",
    "        '''\n",
    "        # Предыдущее значение функции ценности\n",
    "        valueFunctionVector = self.state_values\n",
    "        for iterations in range(self.maxNumberOfIterations):\n",
    "            # Новое значение функции ценности\n",
    "            valueFunctionVectorNextIteration=np.zeros(shape=(self.observation_dim))\n",
    "            # Цикл по состояниям\n",
    "            for state in range(self.observation_dim):\n",
    "                # Вероятности действий\n",
    "                action_probabilities = self.policy_probs[state]\n",
    "                # Цикл по действиям\n",
    "                outerSum=0\n",
    "                for action, prob in enumerate(action_probabilities):\n",
    "                    innerSum=0\n",
    "                    # Цикл по вероятностям действий\n",
    "                    for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
    "                        innerSum=innerSum+probability*(reward+self.gamma*self.state_values[next_state])\n",
    "                    outerSum=outerSum+self.policy_probs[state][action]*innerSum\n",
    "                valueFunctionVectorNextIteration[state]=outerSum\n",
    "            if(np.max(np.abs(valueFunctionVectorNextIteration-valueFunctionVector))<self.theta):\n",
    "                # Проверка сходимости алгоритма\n",
    "                valueFunctionVector=valueFunctionVectorNextIteration\n",
    "                break\n",
    "            valueFunctionVector=valueFunctionVectorNextIteration\n",
    "        return valueFunctionVector               \n",
    "\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        '''\n",
    "        Улучшение стратегии \n",
    "        '''\n",
    "        qvaluesMatrix=np.zeros((self.observation_dim, len(self.actions_variants)))\n",
    "        improvedPolicy=np.zeros((self.observation_dim, len(self.actions_variants)))\n",
    "        # Цикл по состояниям\n",
    "        for state in range(self.observation_dim):\n",
    "            for action in range(len(self.actions_variants)):\n",
    "                for probability, next_state, reward, isTerminalState in self.env.P[state][action]:\n",
    "                    qvaluesMatrix[state,action]=qvaluesMatrix[state,action]+probability*(reward+self.gamma*self.state_values[next_state])\n",
    "                \n",
    "            # Находим лучшие индексы\n",
    "            bestActionIndex=np.where(qvaluesMatrix[state,:]==np.max(qvaluesMatrix[state,:]))\n",
    "            # Обновление стратегии\n",
    "            improvedPolicy[state,bestActionIndex]=1/np.size(bestActionIndex)\n",
    "        return improvedPolicy    \n",
    "\n",
    "\n",
    "    def policy_iteration(self, cnt):\n",
    "        '''\n",
    "        Основная реализация алгоритма\n",
    "        '''\n",
    "        policy_stable = False\n",
    "        for i in range(1, cnt+1):\n",
    "            self.state_values = self.policy_evaluation()\n",
    "            self.policy_probs = self.policy_improvement()\n",
    "        print(f'Алгоритм выполнился за {i} шагов.')\n",
    "\n",
    "\n",
    "def play_agent(agent):\n",
    "    env2 = gym.make('FrozenLake-v1', render_mode='human')\n",
    "    state = env2.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        p = agent.policy_probs[state]\n",
    "        if isinstance(p, np.ndarray):\n",
    "            action = np.random.choice(len(agent.actions_variants), p=p)\n",
    "        else:\n",
    "            action = p\n",
    "        next_state, reward, terminated, truncated, _ = env2.step(action)\n",
    "        env2.render()\n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Создание среды\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "    env.reset()\n",
    "    # Обучение агента\n",
    "    agent = PolicyIterationAgent(env)\n",
    "    agent.print_policy()\n",
    "    agent.policy_iteration(1000)\n",
    "    agent.print_policy()\n",
    "    # Проигрывание сцены для обученного агента\n",
    "    play_agent(agent)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
